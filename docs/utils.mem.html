---

title: utils.mem
keywords: 
sidebar: home_sidebar


---

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Memory-management-utils">Memory management utils<a class="anchor-link" href="#Memory-management-utils">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Utility functions for memory management. Currently primarily for GPU.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gpu_mem_get"><code>gpu_mem_get</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L52" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gpu_mem_get</code>(<strong><code>id</code></strong>=<strong><em><code>None</code></em></strong>)</p>
</blockquote>
<p>get total, used and free memory (in MBs) for gpu <code>id</code>. if <code>id</code> is not passed, currently selected torch device is used</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/utils.mem.html#gpu_mem_get"><code>gpu_mem_get</code></a></p>
<ul>
<li>for gpu returns <code>GPUMemory(total, free, used)</code></li>
<li>for cpu returns <code>GPUMemory(0, 0, 0)</code></li>
<li>for invalid gpu id returns <code>GPUMemory(0, 0, 0)</code></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gpu_mem_get_all"><code>gpu_mem_get_all</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L63" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gpu_mem_get_all</code>()</p>
</blockquote>
<p>get total, used and free memory (in MBs) for each available gpu</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/utils.mem.html#gpu_mem_get_all"><code>gpu_mem_get_all</code></a></p>
<ul>
<li>for gpu returns <code>[ GPUMemory(total_0, free_0, used_0), GPUMemory(total_1, free_1, used_1), .... ]</code></li>
<li>for cpu returns <code>[]</code></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gpu_mem_get_free"><code>gpu_mem_get_free</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L68" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gpu_mem_get_free</code>()</p>
</blockquote>
<p>get free memory (in MBs) for the currently selected gpu id, w/o emptying the cache</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gpu_mem_get_free_no_cache"><code>gpu_mem_get_free_no_cache</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L72" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gpu_mem_get_free_no_cache</code>()</p>
</blockquote>
<p>get free memory (in MBs) for the currently selected gpu id, after emptying the cache</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gpu_mem_get_used"><code>gpu_mem_get_used</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L77" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gpu_mem_get_used</code>()</p>
</blockquote>
<p>get used memory (in MBs) for the currently selected gpu id, w/o emptying the cache</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gpu_mem_get_used_no_cache"><code>gpu_mem_get_used_no_cache</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L86" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gpu_mem_get_used_no_cache</code>()</p>
</blockquote>
<p>get used memory (in MBs) for the currently selected gpu id, after emptying the cache</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/utils.mem.html#gpu_mem_get_used_no_cache"><code>gpu_mem_get_used_no_cache</code></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gpu_mem_get_used_fast"><code>gpu_mem_get_used_fast</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L81" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gpu_mem_get_used_fast</code>(<strong><code>gpu_handle</code></strong>)</p>
</blockquote>
<p>get used memory (in MBs) for the currently selected gpu id, w/o emptying the cache, and needing the <code>gpu_handle</code> arg</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/utils.mem.html#gpu_mem_get_used_fast"><code>gpu_mem_get_used_fast</code></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gpu_with_max_free_mem"><code>gpu_with_max_free_mem</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L91" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gpu_with_max_free_mem</code>()</p>
</blockquote>
<p>get [gpu_id, its_free_ram] for the first gpu with highest available RAM</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/utils.mem.html#gpu_with_max_free_mem"><code>gpu_with_max_free_mem</code></a>:</p>
<ul>
<li>for gpu returns: <code>gpu_with_max_free_ram_id, its_free_ram</code></li>
<li>for cpu returns: <code>None, 0</code></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="preload_pytorch"><code>preload_pytorch</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L45" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>preload_pytorch</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/utils.mem.html#preload_pytorch"><code>preload_pytorch</code></a> is helpful when GPU memory is being measured, since the first time any operation on <code>cuda</code> is performed by pytorch, usually about 0.5GB gets used by CUDA context.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GPUMemory"><code>class</code> <code>GPUMemory</code></h4><blockquote><p><code>GPUMemory</code>(<strong><code>total</code></strong>, <strong><code>free</code></strong>, <strong><code>used</code></strong>) :: <code>tuple</code></p>
</blockquote>
<p>GPUMemory(total, free, used)</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/utils.mem.html#GPUMemory"><code>GPUMemory</code></a> is a namedtuple that is returned by functions like <a href="/utils.mem.html#gpu_mem_get"><code>gpu_mem_get</code></a> and <a href="/utils.mem.html#gpu_mem_get_all"><code>gpu_mem_get_all</code></a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="b2mb"><code>b2mb</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L48" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>b2mb</code>(<strong><code>num</code></strong>)</p>
</blockquote>
<p>convert Bs to MBs and round down</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/utils.mem.html#b2mb"><code>b2mb</code></a> is a helper utility that just does <code>int(bytes/2**20)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Memory-Tracing-Utils">Memory Tracing Utils<a class="anchor-link" href="#Memory-Tracing-Utils">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GPUMemTrace"><code>class</code> <code>GPUMemTrace</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L136" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GPUMemTrace</code>(<strong><code>silent</code></strong>=<strong><em><code>False</code></em></strong>)</p>
</blockquote>
<p>Trace allocated and peaked GPU memory usage (deltas).</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Definitions</strong>:</p>
<ul>
<li><p><strong>Delta Used</strong> is the difference between current used memory and used memory at the start of the counter.</p>
</li>
<li><p><strong>Delta Peaked</strong> is the memory overhead if any. It's calculated in two steps:</p>
<ol>
<li>The base measurement is the difference between the peak memory and the used memory at the start of the counter.</li>
<li><p>Then if delta used is positive it gets subtracted from the base value.</p>
<p>It indicates the size of the blip.</p>
<p><strong>Warning</strong>: currently the peak memory usage tracking is implemented using a python thread, which is very unreliable, since there is no guarantee the thread will get a chance at running at the moment the peak memory is occuring (or it might not get a chance to run at all). Therefore we need pytorch to implement multiple concurrent and resettable <a href="https://pytorch.org/docs/stable/cuda.html#torch.cuda.max_memory_allocated"><code>torch.cuda.max_memory_allocated</code></a> counters. Please vote for this <a href="https://github.com/pytorch/pytorch/issues/16266">feature request</a>.</p>
</li>
</ol>
</li>
</ul>
<p><strong>Usage Examples</strong>:</p>
<p>Setup:</p>

<pre><code>from fastai.utils.mem import GPUMemTrace
def some_code(): pass
mtrace = GPUMemTrace()</code></pre>
<p>Example 1: basic measurements via <code>report</code> (prints) and via <a href="/tabular.data.html#tabular.data"><code>data</code></a> (returns) accessors</p>

<pre><code>some_code()
mtrace.report()
delta_used, delta_peaked = mtrace.data()

some_code()
mtrace.report('2nd run of some_code()')
delta_used, delta_peaked = mtrace.data()</code></pre>
<p><code>report</code>'s optional <code>note</code> argument can be helpful if you have many <code>report</code> calls and you want to understand which is which in the outputs.</p>
<p>Example 2: measure in a loop, resetting the counter before each run</p>

<pre><code>for i in range(10):
    mtrace.reset()
    some_code()
    mtrace.report(f'i={i}') # report for just the last code run since reset</code></pre>
<p><code>reset</code> resets all the counters.</p>
<p>Example 3: like example 2, but having <code>report</code> automatically reset the counters</p>

<pre><code>mtrace.reset()
for i in range(10):
    some_code()
    mtrace.report_n_reset(f'i={i}') # report for just the last code run since reset</code></pre>
<p>The tracing starts immediately upon the <a href="/utils.mem.html#GPUMemTrace"><code>GPUMemTrace</code></a> object creation, and stops when that object is deleted. But it can also be <code>stop</code>ed, <code>start</code>ed manually as well.</p>

<pre><code>mtrace.start()
mtrace.stop()</code></pre>
<p><code>stop</code> is in particular useful if you want to <strong>freeze</strong> the <a href="/utils.mem.html#GPUMemTrace"><code>GPUMemTrace</code></a> object and to be able to query its data on <code>stop</code> some time down the road.</p>
<p><strong>Context Manager</strong>:</p>
<p><a href="/utils.mem.html#GPUMemTrace"><code>GPUMemTrace</code></a> can also be used as a context manager:</p>

<pre><code>with GPUMemTrace() as mtrace:
    some_code()
delta_used, delta_peaked = mtrace.data()
mem_trace.report("measured in ctx")</code></pre>
<p>or just:</p>

<pre><code>with GPUMemTrace() as mtrace:
    some_code()
print(mtrace)</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Workarounds-to-the-leaky-ipython-traceback-on-exception">Workarounds to the leaky ipython traceback on exception<a class="anchor-link" href="#Workarounds-to-the-leaky-ipython-traceback-on-exception">&#182;</a></h2><p>ipython has a feature where it stores tb with all the <code>locals()</code> tied in, which
prevents <code>gc.collect()</code> from freeing those variables and leading to a leakage.</p>
<p>Therefore we cleanse the tb before handing it over to ipython. The 2 ways of doing it are by either using the <a href="/utils.mem.html#gpu_mem_restore"><code>gpu_mem_restore</code></a> decorator or the <a href="/utils.mem.html#gpu_mem_restore_ctx"><code>gpu_mem_restore_ctx</code></a> context manager which are described next:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gpu_mem_restore"><code>gpu_mem_restore</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L105" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gpu_mem_restore</code>(<strong><code>func</code></strong>)</p>
</blockquote>
<p>Reclaim GPU RAM if CUDA out of memory happened, or execution was interrupted</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/utils.mem.html#gpu_mem_restore"><code>gpu_mem_restore</code></a> is a decorator to be used with any functions that interact with CUDA (top-level is fine)</p>
<ul>
<li>under non-ipython environment it doesn't do anything.</li>
<li>under ipython currently it strips tb by default only for the "CUDA out of memory" exception.</li>
</ul>
<p>The env var <code>FASTAI_TB_CLEAR_FRAMES</code> changes this behavior when run under ipython,
depending on its value:</p>
<ul>
<li>"0": never  strip tb (makes it possible to always use <code>%debug</code> magic, but with leaks)</li>
<li>"1": always strip tb (never need to worry about leaks, but <code>%debug</code> won't work)</li>
</ul>
<p>e.g. <code>os.environ['FASTAI_TB_CLEAR_FRAMES']="0"</code> will set it to 0.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gpu_mem_restore_ctx"><code>class</code> <code>gpu_mem_restore_ctx</code><a href="https://github.com/fastai/fastai/blob/master/fastai/utils/mem.py#L127" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gpu_mem_restore_ctx</code>()</p>
</blockquote>
<p>context manager to reclaim RAM if an exception happened under ipython</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>if function decorator is not a good option, you can use a context manager instead. For example:</p>

<pre><code>with gpu_mem_restore_ctx():
   learn.fit_one_cycle(1,1e-2)</code></pre>
<p>This particular one will clear tb on any exception.</p>

</div>
</div>
</div>
</div>
 

